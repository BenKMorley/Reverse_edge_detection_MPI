--------------------------------------------------------------------------------
*** bkm1n   Job: 6236873.sdb   started: 27/05/19 13:44:48   host: mom3 ***
*** bkm1n   Job: 6236873.sdb   started: 27/05/19 13:44:48   host: mom3 ***
*** bkm1n   Job: 6236873.sdb   started: 27/05/19 13:44:48   host: mom3 ***
*** bkm1n   Job: 6236873.sdb   started: 27/05/19 13:44:48   host: mom3 ***

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
Running MPI program parallel2 on 1 processes
Started at Mon May 27 13:44:58 BST 2019
--------------------------------------------------------------------------------

real	0m4.034s
user	0m0.492s
sys	0m0.124s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 2 processes
Started at Mon May 27 13:45:02 BST 2019
--------------------------------------------------------------------------------

real	0m5.337s
user	0m0.512s
sys	0m0.140s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 3 processes
Started at Mon May 27 13:45:07 BST 2019
--------------------------------------------------------------------------------

real	0m5.239s
user	0m0.420s
sys	0m0.180s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 4 processes
Started at Mon May 27 13:45:13 BST 2019
--------------------------------------------------------------------------------

real	0m5.009s
user	0m0.452s
sys	0m0.152s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 6 processes
Started at Mon May 27 13:45:18 BST 2019
--------------------------------------------------------------------------------

real	0m4.984s
user	0m0.436s
sys	0m0.168s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 8 processes
Started at Mon May 27 13:45:23 BST 2019
--------------------------------------------------------------------------------

real	0m4.849s
user	0m0.500s
sys	0m0.112s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 12 processes
Started at Mon May 27 13:45:27 BST 2019
--------------------------------------------------------------------------------

real	0m5.782s
user	0m0.484s
sys	0m0.116s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 16 processes
Started at Mon May 27 13:45:33 BST 2019
--------------------------------------------------------------------------------

real	0m4.815s
user	0m0.472s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 24 processes
Started at Mon May 27 13:45:38 BST 2019
--------------------------------------------------------------------------------

real	0m4.982s
user	0m0.448s
sys	0m0.164s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 1 processes
Started at Mon May 27 13:45:43 BST 2019
--------------------------------------------------------------------------------

real	0m9.361s
user	0m0.452s
sys	0m0.164s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 2 processes
Started at Mon May 27 13:45:52 BST 2019
--------------------------------------------------------------------------------

real	0m6.317s
user	0m0.496s
sys	0m0.100s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 3 processes
Started at Mon May 27 13:45:59 BST 2019
--------------------------------------------------------------------------------
Rank 0 [Mon May 27 13:46:03 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 2 [Mon May 27 13:46:03 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 1 [Mon May 27 13:46:03 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
_pmiu_daemon(SIGCHLD): [NID 01161] [c6-0c0s2n1] [Mon May 27 13:46:03 2019] PE RANK 2 exit signal Aborted

real	0m4.539s
user	0m0.428s
sys	0m0.176s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 4 processes
Started at Mon May 27 13:46:03 BST 2019
--------------------------------------------------------------------------------

real	0m5.590s
user	0m0.480s
sys	0m0.124s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 6 processes
Started at Mon May 27 13:46:09 BST 2019
--------------------------------------------------------------------------------
Rank 3 [Mon May 27 13:46:13 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 2 [Mon May 27 13:46:13 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 0 [Mon May 27 13:46:13 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 4 [Mon May 27 13:46:13 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 1 [Mon May 27 13:46:13 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 5 [Mon May 27 13:46:13 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
_pmiu_daemon(SIGCHLD): [NID 01161] [c6-0c0s2n1] [Mon May 27 13:46:13 2019] PE RANK 3 exit signal Aborted

real	0m4.622s
user	0m0.444s
sys	0m0.160s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 8 processes
Started at Mon May 27 13:46:14 BST 2019
--------------------------------------------------------------------------------

real	0m5.175s
user	0m0.476s
sys	0m0.132s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 12 processes
Started at Mon May 27 13:46:19 BST 2019
--------------------------------------------------------------------------------

real	0m5.024s
user	0m0.480s
sys	0m0.136s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 16 processes
Started at Mon May 27 13:46:24 BST 2019
--------------------------------------------------------------------------------

real	0m5.103s
user	0m0.444s
sys	0m0.152s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 24 processes
Started at Mon May 27 13:46:29 BST 2019
--------------------------------------------------------------------------------

real	0m4.939s
user	0m0.444s
sys	0m0.148s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 1 processes
Started at Mon May 27 13:46:34 BST 2019
--------------------------------------------------------------------------------

real	0m29.781s
user	0m0.480s
sys	0m0.132s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 2 processes
Started at Mon May 27 13:47:04 BST 2019
--------------------------------------------------------------------------------

real	0m17.794s
user	0m0.456s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 3 processes
Started at Mon May 27 13:47:21 BST 2019
--------------------------------------------------------------------------------
Rank 1 [Mon May 27 13:47:26 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 2 [Mon May 27 13:47:26 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 0 [Mon May 27 13:47:26 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
_pmiu_daemon(SIGCHLD): [NID 01161] [c6-0c0s2n1] [Mon May 27 13:47:26 2019] PE RANK 1 exit signal Aborted

real	0m5.189s
user	0m0.460s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 4 processes
Started at Mon May 27 13:47:27 BST 2019
--------------------------------------------------------------------------------

real	0m11.715s
user	0m0.448s
sys	0m0.152s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 6 processes
Started at Mon May 27 13:47:38 BST 2019
--------------------------------------------------------------------------------
Rank 5 [Mon May 27 13:47:43 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 1 [Mon May 27 13:47:43 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 4 [Mon May 27 13:47:43 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 3 [Mon May 27 13:47:43 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 2 [Mon May 27 13:47:43 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
Rank 0 [Mon May 27 13:47:43 2019] [c6-0c0s2n1] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff7210) failed
PMPI_Comm_rank(67).: Null communicator
_pmiu_daemon(SIGCHLD): [NID 01161] [c6-0c0s2n1] [Mon May 27 13:47:43 2019] PE RANK 5 exit signal Aborted
[NID 01161] 2019-05-27 13:47:43 Apid 36025238: initiated application termination

real	0m4.615s
user	0m0.464s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 8 processes
Started at Mon May 27 13:47:43 BST 2019
--------------------------------------------------------------------------------

real	0m8.637s
user	0m0.432s
sys	0m0.176s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 12 processes
Started at Mon May 27 13:47:52 BST 2019
--------------------------------------------------------------------------------

real	0m7.815s
user	0m0.436s
sys	0m0.172s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 16 processes
Started at Mon May 27 13:48:00 BST 2019
--------------------------------------------------------------------------------

real	0m5.032s
user	0m0.488s
sys	0m0.152s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 24 processes
Started at Mon May 27 13:48:05 BST 2019
--------------------------------------------------------------------------------

real	0m6.291s
user	0m0.464s
sys	0m0.140s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 1 processes
Started at Mon May 27 13:48:11 BST 2019
--------------------------------------------------------------------------------

real	1m40.507s
user	0m0.488s
sys	0m0.132s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 2 processes
Started at Mon May 27 13:49:51 BST 2019
--------------------------------------------------------------------------------

real	0m54.332s
user	0m0.440s
sys	0m0.168s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 3 processes
Started at Mon May 27 13:50:46 BST 2019
--------------------------------------------------------------------------------

real	0m38.860s
user	0m0.496s
sys	0m0.108s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 4 processes
Started at Mon May 27 13:51:25 BST 2019
--------------------------------------------------------------------------------

real	0m31.267s
user	0m0.488s
sys	0m0.136s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 6 processes
Started at Mon May 27 13:51:56 BST 2019
--------------------------------------------------------------------------------

real	0m23.705s
user	0m0.448s
sys	0m0.156s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 8 processes
Started at Mon May 27 13:52:20 BST 2019
--------------------------------------------------------------------------------

real	0m19.131s
user	0m0.444s
sys	0m0.164s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 12 processes
Started at Mon May 27 13:52:39 BST 2019
--------------------------------------------------------------------------------

real	0m14.478s
user	0m0.460s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 16 processes
Started at Mon May 27 13:52:53 BST 2019
--------------------------------------------------------------------------------

real	0m12.354s
user	0m0.484s
sys	0m0.120s
--------------------------------------------------------------------------------
Running MPI program parallel2 on 24 processes
Started at Mon May 27 13:53:06 BST 2019
--------------------------------------------------------------------------------

real	0m9.960s
user	0m0.468s
sys	0m0.144s
--------------------------------------------------------------------------------
Finished at Mon May 27 13:53:16 BST 2019
--------------------------------------------------------------------------------

Resources requested: ncpus=24,place=free,walltime=00:10:00
Resources allocated: cpupercent=8,cput=00:00:27,mem=21180kb,ncpus=24,vmem=313652kb,walltime=00:08:28

*** bkm1n   Job: 6236873.sdb   ended: 27/05/19 13:53:16   queue: standard ***
*** bkm1n   Job: 6236873.sdb   ended: 27/05/19 13:53:16   queue: standard ***
*** bkm1n   Job: 6236873.sdb   ended: 27/05/19 13:53:16   queue: standard ***
*** bkm1n   Job: 6236873.sdb   ended: 27/05/19 13:53:16   queue: standard ***
--------------------------------------------------------------------------------
