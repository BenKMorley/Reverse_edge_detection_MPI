--------------------------------------------------------------------------------
*** bkm1n   Job: 6236972.sdb   started: 27/05/19 14:57:34   host: mom2 ***
*** bkm1n   Job: 6236972.sdb   started: 27/05/19 14:57:34   host: mom2 ***
*** bkm1n   Job: 6236972.sdb   started: 27/05/19 14:57:34   host: mom2 ***
*** bkm1n   Job: 6236972.sdb   started: 27/05/19 14:57:34   host: mom2 ***

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
Running MPI program parallel on 1 processes
Started at Mon May 27 14:57:44 BST 2019
--------------------------------------------------------------------------------

real	0m3.073s
user	0m0.428s
sys	0m0.188s
--------------------------------------------------------------------------------
Running MPI program parallel on 2 processes
Started at Mon May 27 14:57:47 BST 2019
--------------------------------------------------------------------------------

real	0m6.266s
user	0m0.460s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel on 3 processes
Started at Mon May 27 14:57:53 BST 2019
--------------------------------------------------------------------------------

real	0m4.903s
user	0m0.452s
sys	0m0.160s
--------------------------------------------------------------------------------
Running MPI program parallel on 4 processes
Started at Mon May 27 14:57:58 BST 2019
--------------------------------------------------------------------------------

real	0m4.842s
user	0m0.460s
sys	0m0.148s
--------------------------------------------------------------------------------
Running MPI program parallel on 6 processes
Started at Mon May 27 14:58:03 BST 2019
--------------------------------------------------------------------------------

real	0m4.837s
user	0m0.424s
sys	0m0.192s
--------------------------------------------------------------------------------
Running MPI program parallel on 8 processes
Started at Mon May 27 14:58:08 BST 2019
--------------------------------------------------------------------------------

real	0m4.733s
user	0m0.460s
sys	0m0.148s
--------------------------------------------------------------------------------
Running MPI program parallel on 12 processes
Started at Mon May 27 14:58:12 BST 2019
--------------------------------------------------------------------------------

real	0m4.804s
user	0m0.464s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel on 16 processes
Started at Mon May 27 14:58:17 BST 2019
--------------------------------------------------------------------------------

real	0m4.984s
user	0m0.504s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel on 24 processes
Started at Mon May 27 14:58:24 BST 2019
--------------------------------------------------------------------------------

real	0m3.348s
user	0m0.452s
sys	0m0.140s
--------------------------------------------------------------------------------
Running MPI program parallel on 1 processes
Started at Mon May 27 14:58:28 BST 2019
--------------------------------------------------------------------------------

real	0m5.175s
user	0m0.472s
sys	0m0.156s
--------------------------------------------------------------------------------
Running MPI program parallel on 2 processes
Started at Mon May 27 14:58:33 BST 2019
--------------------------------------------------------------------------------

real	0m5.376s
user	0m0.452s
sys	0m0.160s
--------------------------------------------------------------------------------
Running MPI program parallel on 3 processes
Started at Mon May 27 14:58:38 BST 2019
--------------------------------------------------------------------------------
Rank 1 [Mon May 27 14:58:43 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 2 [Mon May 27 14:58:43 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 0 [Mon May 27 14:58:43 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
_pmiu_daemon(SIGCHLD): [NID 02928] [c7-1c0s12n0] [Mon May 27 14:58:43 2019] PE RANK 1 exit signal Aborted

real	0m4.679s
user	0m0.456s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel on 4 processes
Started at Mon May 27 14:58:43 BST 2019
--------------------------------------------------------------------------------

real	0m4.832s
user	0m0.436s
sys	0m0.176s
--------------------------------------------------------------------------------
Running MPI program parallel on 6 processes
Started at Mon May 27 14:58:48 BST 2019
--------------------------------------------------------------------------------
Rank 3 [Mon May 27 14:58:52 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 5 [Mon May 27 14:58:52 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 4 [Mon May 27 14:58:52 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 2 [Mon May 27 14:58:52 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 0 [Mon May 27 14:58:52 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 1 [Mon May 27 14:58:52 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
_pmiu_daemon(SIGCHLD): [NID 02928] [c7-1c0s12n0] [Mon May 27 14:58:52 2019] PE RANK 3 exit signal Aborted
[NID 02928] 2019-05-27 14:58:52 Apid 36026310: initiated application termination

real	0m4.684s
user	0m0.456s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel on 8 processes
Started at Mon May 27 14:58:53 BST 2019
--------------------------------------------------------------------------------

real	0m5.091s
user	0m0.464s
sys	0m0.128s
--------------------------------------------------------------------------------
Running MPI program parallel on 12 processes
Started at Mon May 27 14:58:58 BST 2019
--------------------------------------------------------------------------------

real	0m4.916s
user	0m0.420s
sys	0m0.188s
--------------------------------------------------------------------------------
Running MPI program parallel on 16 processes
Started at Mon May 27 14:59:03 BST 2019
--------------------------------------------------------------------------------

real	0m4.796s
user	0m0.432s
sys	0m0.172s
--------------------------------------------------------------------------------
Running MPI program parallel on 24 processes
Started at Mon May 27 14:59:08 BST 2019
--------------------------------------------------------------------------------

real	0m4.764s
user	0m0.468s
sys	0m0.124s
--------------------------------------------------------------------------------
Running MPI program parallel on 1 processes
Started at Mon May 27 14:59:12 BST 2019
--------------------------------------------------------------------------------

real	0m8.505s
user	0m0.468s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel on 2 processes
Started at Mon May 27 14:59:21 BST 2019
--------------------------------------------------------------------------------

real	0m6.748s
user	0m0.488s
sys	0m0.128s
--------------------------------------------------------------------------------
Running MPI program parallel on 3 processes
Started at Mon May 27 14:59:28 BST 2019
--------------------------------------------------------------------------------
Rank 1 [Mon May 27 14:59:32 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 2 [Mon May 27 14:59:32 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 0 [Mon May 27 14:59:32 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
_pmiu_daemon(SIGCHLD): [NID 02928] [c7-1c0s12n0] [Mon May 27 14:59:32 2019] PE RANK 1 exit signal Aborted

real	0m7.719s
user	0m0.444s
sys	0m0.164s
--------------------------------------------------------------------------------
Running MPI program parallel on 4 processes
Started at Mon May 27 14:59:35 BST 2019
--------------------------------------------------------------------------------

real	0m4.006s
user	0m0.456s
sys	0m0.172s
--------------------------------------------------------------------------------
Running MPI program parallel on 6 processes
Started at Mon May 27 14:59:39 BST 2019
--------------------------------------------------------------------------------
Rank 3 [Mon May 27 14:59:44 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 5 [Mon May 27 14:59:44 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 1 [Mon May 27 14:59:44 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 4 [Mon May 27 14:59:44 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 0 [Mon May 27 14:59:44 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
Rank 2 [Mon May 27 14:59:44 2019] [c7-1c0s12n0] Fatal error in PMPI_Comm_rank: Invalid communicator, error stack:
PMPI_Comm_rank(110): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffffff71f0) failed
PMPI_Comm_rank(67).: Null communicator
_pmiu_daemon(SIGCHLD): [NID 02928] [c7-1c0s12n0] [Mon May 27 14:59:44 2019] PE RANK 3 exit signal Aborted

real	0m4.868s
user	0m0.460s
sys	0m0.160s
--------------------------------------------------------------------------------
Running MPI program parallel on 8 processes
Started at Mon May 27 14:59:44 BST 2019
--------------------------------------------------------------------------------

real	0m6.785s
user	0m0.468s
sys	0m0.156s
--------------------------------------------------------------------------------
Running MPI program parallel on 12 processes
Started at Mon May 27 14:59:51 BST 2019
--------------------------------------------------------------------------------

real	0m3.409s
user	0m0.448s
sys	0m0.172s
--------------------------------------------------------------------------------
Running MPI program parallel on 16 processes
Started at Mon May 27 14:59:55 BST 2019
--------------------------------------------------------------------------------

real	0m5.297s
user	0m0.456s
sys	0m0.160s
--------------------------------------------------------------------------------
Running MPI program parallel on 24 processes
Started at Mon May 27 15:00:01 BST 2019
--------------------------------------------------------------------------------

real	0m5.249s
user	0m0.472s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel on 1 processes
Started at Mon May 27 15:00:06 BST 2019
--------------------------------------------------------------------------------

real	0m16.272s
user	0m0.416s
sys	0m0.192s
--------------------------------------------------------------------------------
Running MPI program parallel on 2 processes
Started at Mon May 27 15:00:22 BST 2019
--------------------------------------------------------------------------------

real	0m10.607s
user	0m0.480s
sys	0m0.136s
--------------------------------------------------------------------------------
Running MPI program parallel on 3 processes
Started at Mon May 27 15:00:33 BST 2019
--------------------------------------------------------------------------------

real	0m8.921s
user	0m0.476s
sys	0m0.144s
--------------------------------------------------------------------------------
Running MPI program parallel on 4 processes
Started at Mon May 27 15:00:42 BST 2019
--------------------------------------------------------------------------------

real	0m8.073s
user	0m0.460s
sys	0m0.152s
--------------------------------------------------------------------------------
Running MPI program parallel on 6 processes
Started at Mon May 27 15:00:50 BST 2019
--------------------------------------------------------------------------------

real	0m7.126s
user	0m0.456s
sys	0m0.160s
--------------------------------------------------------------------------------
Running MPI program parallel on 8 processes
Started at Mon May 27 15:00:57 BST 2019
--------------------------------------------------------------------------------

real	0m6.681s
user	0m0.468s
sys	0m0.140s
--------------------------------------------------------------------------------
Running MPI program parallel on 12 processes
Started at Mon May 27 15:01:04 BST 2019
--------------------------------------------------------------------------------

real	0m6.011s
user	0m0.452s
sys	0m0.160s
--------------------------------------------------------------------------------
Running MPI program parallel on 16 processes
Started at Mon May 27 15:01:10 BST 2019
--------------------------------------------------------------------------------

real	0m5.905s
user	0m0.476s
sys	0m0.140s
--------------------------------------------------------------------------------
Running MPI program parallel on 24 processes
Started at Mon May 27 15:01:16 BST 2019
--------------------------------------------------------------------------------

real	0m5.702s
user	0m0.476s
sys	0m0.132s
--------------------------------------------------------------------------------
Finished at Mon May 27 15:01:21 BST 2019
--------------------------------------------------------------------------------

Resources requested: ncpus=24,place=free,walltime=00:10:00
Resources allocated: cpupercent=11,cput=00:00:28,mem=9256kb,ncpus=24,vmem=188604kb,walltime=00:03:47

*** bkm1n   Job: 6236972.sdb   ended: 27/05/19 15:01:21   queue: standard ***
*** bkm1n   Job: 6236972.sdb   ended: 27/05/19 15:01:21   queue: standard ***
*** bkm1n   Job: 6236972.sdb   ended: 27/05/19 15:01:21   queue: standard ***
*** bkm1n   Job: 6236972.sdb   ended: 27/05/19 15:01:21   queue: standard ***
--------------------------------------------------------------------------------
